{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0d364ed-a09e-4873-a87a-38afd141b1e5",
   "metadata": {},
   "source": [
    "### Installing the required module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8616f6b7-a9b6-4ac6-b245-c4a9bc8699c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\deeps\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d897ab-cafe-4334-aff5-64a3e0a05622",
   "metadata": {},
   "source": [
    "## Loading the Punkt model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c8c46f-f5a3-4937-bfb8-7f6bb9031843",
   "metadata": {},
   "source": [
    "Punkt is pre trained model tokenizer model used for word or sentence tokenize.\n",
    "1. Language Independent \n",
    "2. Handles Abbreavtions (D.R,E.g)\n",
    "3. Customization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2f18983-15a8-4166-bb07-dee2f3911f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\deeps\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a25cc2d-c3a0-4126-9f19-a360627a6957",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt= \"Hey,how are you i am learning the concepts of the NLP from the GFG Data Science program\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0b8cce6-c509-4bc2-a09b-830e57bbc718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey,how',\n",
       " 'are',\n",
       " 'you',\n",
       " 'i',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'the',\n",
       " 'concepts',\n",
       " 'of',\n",
       " 'the',\n",
       " 'NLP',\n",
       " 'from',\n",
       " 'the',\n",
       " 'GFG',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'program']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdb49511-7f51-4978-9e8a-42a1a3dee024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb8d1c66-d293-4ea9-9834-8977913b9ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey',\n",
       " ',',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " 'i',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'the',\n",
       " 'concepts',\n",
       " 'of',\n",
       " 'the',\n",
       " 'NLP',\n",
       " 'from',\n",
       " 'the',\n",
       " 'GFG',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'program']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0fad23-6e43-4dd2-a71b-502db6e04fec",
   "metadata": {},
   "source": [
    "#### IN the model comma is treated as differnt word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3666ccdc-4eaa-4281-a58b-65f469cc5edf",
   "metadata": {},
   "source": [
    "## Stemming & Lematization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681cd1d5-9519-4bdf-8d0b-cadc2a276bcc",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "<b>Definition:</b> Reduces a word to its root or base form, often by chopping off suffixes or prefixes.<br>\n",
    "<b>Methodology:</b> Uses rule-based algorithms without considering the context or grammatical correctness.<br>\n",
    "<b>Output:</b> May not always produce a valid word.\n",
    "Example:\n",
    "\"playing\" → \"play\"\n",
    "\"played\" → \"play\"\n",
    "\"happily\" → \"happi\"<br>\n",
    "<b>Common Stemming Algorithms</b>\n",
    "1. Porter Stemmer: A widely used rule-based approach.\n",
    "2. Lancaster Stemmer: More aggressive than Porter.\n",
    "3. Snowball Stemmer: An improvement over the Porter Stemmer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022e659b-0025-444b-b0c2-8755cc9a98e0",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "<b>Definition:</b> Reduces a word to its lemma (dictionary form) based on its meaning and grammatical context.<br>\n",
    "<b>Methodology:</b> Uses vocabulary (dictionary) and morphological analysis.<br>\n",
    "<b>Output:</b> Always produces a valid word.<br>\n",
    "Example:\n",
    "1. \"playing\" → \"play\"\n",
    "2. \"better\" → \"good\"\n",
    "3. \"wolves\" → \"wolf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca46ba5-59a8-4bd0-ae85-8e7262ade785",
   "metadata": {},
   "source": [
    "#### Importing the wordnet and omvi1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62d091e-d2d1-487c-a149-19c76025eae0",
   "metadata": {},
   "source": [
    "WordNet is a lexical database for the English language, widely used in Natural Language Processing (NLP) tasks. It groups words into synsets (sets of synonyms), providing semantic relationships between them, such as hypernyms, hyponyms, meronyms, and antonyms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd532c7-ff8f-41da-b1bc-91dc99a526c7",
   "metadata": {},
   "source": [
    "OMV (Ontology Metadata Vocabulary) is a standard framework used to define metadata for ontologies. It allows the description and sharing of semantic information about ontologies, aiding in ontology discovery, selection, and reuse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14063ce8-9f35-49e4-b8c5-17d8360de547",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\deeps\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\deeps\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94e53750-585f-403d-b87f-d04939dfaf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem=PorterStemmer()\n",
    "lam=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4423a376-4c16-4c7d-9df1-d779a989aa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change\n",
      "change\n",
      "changer\n",
      "changed\n"
     ]
    }
   ],
   "source": [
    "print(lam.lemmatize('change'))\n",
    "print(lam.lemmatize('changes'))\n",
    "print(lam.lemmatize('changer'))\n",
    "print(lam.lemmatize('changed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "612197fc-a47f-4811-8138-ba6cafe3db77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chang\n",
      "chang\n",
      "chang\n"
     ]
    }
   ],
   "source": [
    "print(stem.stem('change'))\n",
    "print(stem.stem('changes'))\n",
    "print(stem.stem('changed'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0276c529-604d-4da3-8339-ea163cb57de0",
   "metadata": {},
   "source": [
    "### Dealing with the Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58089faf-6e36-491f-b8ff-a90a435b745e",
   "metadata": {},
   "source": [
    "Stopwords in Natural Language Processing (NLP) are common words that are typically removed from text data because they contribute little to the meaning of the text. Examples of stopwords include \"a,\" \"an,\" \"the,\" \"is,\" \"in,\" and \"on.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b566ccc6-1eda-44a0-bfa5-cc8c2d76d22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\deeps\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b5b7c2a-ee35-4015-bed2-d33a8d828d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=stopwords.words('English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ddf67a8-3391-43c3-919d-1c940237d6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99c52911-f081-4571-8065-1a7e570b9b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt= \"Hey,how are you i am learning the concepts of the NLP from the GFG Data Science program\"\n",
    "txt=word_tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc3415d5-b6b1-472d-8609-7c88f3b16543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey\n",
      "learning\n",
      "concepts\n",
      "NLP\n",
      "GFG\n",
      "Data\n",
      "Science\n",
      "program\n"
     ]
    }
   ],
   "source": [
    "for word in txt:\n",
    "    if ((word.lower() not in stop) and (len(word) != 1)):\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b8a217-6dda-407a-bf7e-05dcbc55e8af",
   "metadata": {},
   "source": [
    "### Corpus and Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e057fee8-bf4d-418e-8a4b-f15a20ef974d",
   "metadata": {},
   "source": [
    "Both corpus and vocabulary are foundational concepts in Natural Language Processing (NLP), serving as essential resources for training and analyzing models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb98db2e-e055-4614-bfe4-0eb3170c8028",
   "metadata": {},
   "source": [
    "1. A corpus (plural: corpora) is a collection of text or spoken material used for linguistic analysis or model training in NLP.\n",
    "\n",
    "<b>Characteristics of a Corpus:</b><br>\n",
    "i.)  <b>Diverse Content:</b> Can include books, articles, conversations, web content, etc.<br>\n",
    "ii.) <b>Language Specificity:</b> May focus on a single language or be multilingual.<br>\n",
    "iii.) <b> Domain-Specific: </b>Specialized corpora exist for fields like law, medicine, or social media.<br>\n",
    "iv.) <b>Size: </b>Modern NLP systems often require large corpora (e.g., billions of words) for effective training.<br>\n",
    "\n",
    "2. The vocabulary is the set of unique words or tokens present in a corpus. It forms the building blocks for NLP tasks.\n",
    "\n",
    "<b>Characteristics of Vocabulary:</b><br>\n",
    "i.)<b>Size:</b> Can vary from small (controlled environments) to large (open-domain corpora).<bt>\n",
    "ii.) <b>Tokens:<b> Includes words, subwords, punctuation, or special symbols.<br>\n",
    "iii) <b>Language-Specific:<b>Each language has its own vocabulary.<br>\n",
    "iv) <b>Dynamic Nature:</b> Vocabulary may evolve as new words (e.g., slang, technical terms) are added.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fce9c639-2dd2-45d9-b4bb-732febdcef3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "corpus='India, officially the Republic of India, is a country in South Asia. It is the seventh-largest country by area, the second-most populous country, and the most populous democracy in the world. Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand, Myanmar and Indonesia.'\n",
    "len(word_tokenize(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b417ca-1b54-465b-8e8a-c5654119c083",
   "metadata": {},
   "source": [
    "#### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "981e62c7-c5c6-4b73-a527-8fb0ad5a4676",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[]\n",
    "\n",
    "for word in word_tokenize(corpus):\n",
    "    if(word.lower() not in stopwords.words('English') and len(word)!=1):\n",
    "        words.append(word.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62b9a2bc-4c3d-469b-8f2f-3ce611694d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1a2e64-3987-49b6-97c9-b9340e325f4b",
   "metadata": {},
   "source": [
    "<b> NOTE:</b> The Stopwords encocuntered in the corpus are 69.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f15d00b-b70a-4c74-b614-6e23a8eab466",
   "metadata": {},
   "source": [
    "##### Sentnces in corpus \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dce4b7ee-ccbb-4bfa-8078-3123be4bd5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India, officially the Republic of India, is a country in South Asia.\n",
      "It is the seventh-largest country by area, the second-most populous country, and the most populous democracy in the world.\n",
      "Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east.\n",
      "In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand, Myanmar and Indonesia.\n"
     ]
    }
   ],
   "source": [
    "for sent in sent_tokenize(corpus):\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dfd2ed-6ecb-415b-b9e0-7e475fce0235",
   "metadata": {},
   "source": [
    "## With KERAS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3ada34-aa0e-4e44-8b1b-844384fea4ff",
   "metadata": {},
   "source": [
    "### Text to Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9cad596-390a-4b85-8163-bd1c294b9cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\deeps\\anaconda3\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.68.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: rich in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\deeps\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tok=Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95016b25-d900-474b-bc69-5b6cad20e4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "corp = ['coffee is hot','water is cold','hot is not cold']\n",
    "\n",
    "tok.fit_on_texts(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8fa5fa-5329-4412-b075-fe9aeb451c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f632bc4d-a1e4-4ba9-8268-6b9e5461569f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 1, 2], [4, 1, 3], [2, 4, 1, 6, 3]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.texts_to_sequences(['water is hot','black coffee is cold','hot coffee is not cold'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155d5e51-9156-47b3-b149-9e9bd5489489",
   "metadata": {},
   "source": [
    "### Adding out of the vocablury word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cce0f368-9254-4c2d-a9f4-bc26d9689001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'black': 1, 'is': 2, 'coffee': 3, 'hot': 4, 'water': 5, 'cold': 6}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[5, 2, 4], [1, 3, 2, 6], [4, 3, 2, 1, 6]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tok = Tokenizer(oov_token = 'black')\n",
    "\n",
    "corp = ['coffee is hot','water is cold']\n",
    "tok.fit_on_texts(corp)\n",
    "print(tok.word_index)\n",
    "\n",
    "tok.texts_to_sequences(['water is hot','black coffee is cold','hot coffee is not cold'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaf535d-92e6-42f3-a68e-40741f035c0e",
   "metadata": {},
   "source": [
    "### Limting the number of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6eaa7180-9466-4a4c-addb-f109dc23694c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 1, 'coffee': 2, 'hot': 3, 'water': 4, 'cold': 5}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[4, 1, 3], [2, 1, 5]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# num_words -1 are selected and also one issaved for <oov> token \n",
    "tok = Tokenizer(num_words = 6)\n",
    "\n",
    "corp = ['coffee is hot','water is cold']\n",
    "tok.fit_on_texts(corp)\n",
    "print(tok.word_index)\n",
    "\n",
    "tok.texts_to_sequences(['water is hot','black coffee is cold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6352fed8-f7a7-4cb3-866c-6568d037c84a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
